{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a044d036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cda2fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cifar10_data():\n",
    "    data = []\n",
    "    labels = []\n",
    "    for batch in range(1, 6):\n",
    "        batch_data = unpickle(f'cifar-10-batches-py/data_batch_{batch}')\n",
    "        data.append(batch_data[b'data'])\n",
    "        labels.extend(batch_data[b'labels'])\n",
    "\n",
    "    data = np.concatenate(data, axis=0)\n",
    "    data = data.reshape(-1, 3, 32, 32)  # Reshape to (num_samples, channels, height, width)\n",
    "    data = np.transpose(data, (0, 2, 3, 1))  # Transpose to (num_samples, height, width, channels)\n",
    "\n",
    "    test_batch = unpickle('cifar-10-batches-py/test_batch')\n",
    "    test_data = test_batch[b'data']\n",
    "    test_labels = test_batch[b'labels']\n",
    "\n",
    "    test_data = test_data.reshape(-1, 3, 32, 32)\n",
    "    test_data = np.transpose(test_data, (0, 2, 3, 1))\n",
    "\n",
    "    # Perform one-hot encoding for labels\n",
    "    num_classes = 10\n",
    "    labels = np.eye(num_classes)[labels]\n",
    "    test_labels = np.eye(num_classes)[test_labels]\n",
    "\n",
    "    return data, labels, test_data, test_labels\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "x_train, y_train, x_test, y_test = load_cifar10_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f336cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape = (50000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data shape = {}\".format(x_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3172db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train and y_train are your CIFAR-10 training data\n",
    "x = x_train.reshape(50000, -1).T\n",
    "y = y_train.reshape(50000, -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253591da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape = (3072, 50000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input data shape = {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba35bd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train and y_train are your CIFAR-10 training data\n",
    "x = x_train.reshape(x_train.shape[0], -1).T\n",
    "y = y_train.reshape(y_train.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3450aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper Functions...\n",
    "\n",
    "def f(x, w, b):\n",
    "    '''Sigmoid Function'''\n",
    "    f = 1 / (1 + np.exp(-(np.dot(w.T, x) + b)))\n",
    "    return f\n",
    "\n",
    "def mse(x, y, w, b):\n",
    "    '''Mean Squared Loss Function'''\n",
    "    L = 0.5 * np.mean((y - f(x, w, b)) ** 2)\n",
    "    return L\n",
    "\n",
    "def cross_entropy(x, y, w, b):\n",
    "    '''Cross Entropy Loss Function'''\n",
    "    L = -np.mean(y * np.log(f(x, w, b)) + (1 - y) * np.log(1 - f(x, w, b)))\n",
    "    return L\n",
    "\n",
    "def grad_w_mse(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    dw = np.dot(x, (fx - y).T) / x.shape[1]\n",
    "    return dw\n",
    "\n",
    "def grad_b_mse(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    db = np.mean(fx - y)\n",
    "    return db\n",
    "\n",
    "def grad_w_cross(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    dw = np.dot(x, (fx - y).T) / x.shape[1]\n",
    "    return dw\n",
    "\n",
    "def grad_b_cross(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    db = np.mean(fx - y)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6144d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mini-Batch Adam...\n",
    "\n",
    "def MiniBatchAdam(x, y, epochs, batch_size, loss, lr):\n",
    "    w = np.random.randn(x.shape[0], 1)\n",
    "    b = np.zeros((1, 1))\n",
    "    epsilon = 1e-8\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    momentum_w, momentum_b = 0, 0\n",
    "    update_w, update_b = 0, 0\n",
    "    l_list = []\n",
    "    w_list = []\n",
    "    b_list = []\n",
    "    ep = [i for i in range(epochs + 1)]\n",
    "\n",
    "    \n",
    "    num_samples = x.shape[1]\n",
    "    num_batches = x.shape[1] // batch_size\n",
    "    if x.shape[1] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # Shuffle the data\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    x = x[:, idx]\n",
    "    y = y[:, idx]\n",
    "\n",
    "    selected_samples = int(num_samples * 0.15)\n",
    "    x = x[:, :selected_samples]\n",
    "    y = y[:, :selected_samples]\n",
    "\n",
    "    for i in range(epochs + 1):\n",
    "        for j in range(num_batches):\n",
    "            start_idx = j * batch_size\n",
    "            end_idx = (j + 1) * batch_size\n",
    "\n",
    "            x_batch = x[:, start_idx:end_idx]\n",
    "            y_batch = y[:, start_idx:end_idx]\n",
    "\n",
    "            dw, db = 0, 0\n",
    "            if loss == 'mse':\n",
    "                dw += grad_w_mse(x_batch, y_batch, w, b)\n",
    "                db += grad_b_mse(x_batch, y_batch, w, b)\n",
    "            elif loss == 'cross_entropy':\n",
    "                dw += grad_w_cross(x_batch, y_batch, w, b)\n",
    "                db += grad_b_cross(x_batch, y_batch, w, b)\n",
    "\n",
    "            # Momentum\n",
    "            momentum_w = beta1 * momentum_w + (1 - beta1) * dw\n",
    "            momentum_b = beta1 * momentum_b + (1 - beta1) * db\n",
    "            # Update History\n",
    "            update_w = beta2 * update_w + (1 - beta2) * dw ** 2\n",
    "            update_b = beta2 * update_b + (1 - beta2) * db ** 2\n",
    "            # Bias Correction\n",
    "            momentum_w = momentum_w / (1 - beta1 ** (i + 1))\n",
    "            momentum_b = momentum_b / (1 - beta1 ** (i + 1))\n",
    "            update_w = update_w / (1 - beta2 ** (i + 1))\n",
    "            update_b = update_b / (1 - beta2 ** (i + 1))\n",
    "            # Update of Parameters\n",
    "            w = w - (lr / (np.sqrt(update_w) + epsilon)) * momentum_w\n",
    "            b = b - (lr / (np.sqrt(update_b) + epsilon)) * momentum_b\n",
    "\n",
    "        if loss == 'mse':\n",
    "            l = mse(x, y, w, b)\n",
    "            print(f'Loss after {i}th epoch = {l}')\n",
    "            l_list.append(l)\n",
    "        elif loss == 'cross_entropy':\n",
    "            l = cross_entropy(x, y, w, b)\n",
    "            print(f'Loss after {i}th epoch = {l}')\n",
    "            l_list.append(l)\n",
    "        w_list.append(w)\n",
    "        b_list.append(b)\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epoch Curve\\nAlgorithm: Mini-Batch Adam\\nBatch Size = {}\\nLearning Rate = {}\\nLoss Function = {}'.format(batch_size, lr, loss))\n",
    "    plt.plot(ep, l_list)\n",
    "    plt.show()\n",
    "\n",
    "    return w_list, b_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a770cfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mini-Batch Adam...\n",
    "\n",
    "def MiniBatchAdam(x, y, epochs, batch_size, loss, lr, l2_reg):\n",
    "    \n",
    "    # Feature Scaling - Min-Max scaling\n",
    "    x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "    w = np.random.randn(x.shape[0], 1) * 0.01\n",
    "    b = np.zeros((1, 1))\n",
    "    epsilon = 1e-8\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    momentum_w, momentum_b = 0, 0\n",
    "    update_w, update_b = 0, 0\n",
    "    l_list = []\n",
    "    w_list = []\n",
    "    b_list = []\n",
    "    ep = [i for i in range(epochs + 1)]\n",
    "\n",
    "    num_samples = x.shape[1]\n",
    "    num_batches = num_samples // batch_size\n",
    "    if num_samples % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # Shuffle the data\n",
    "    idx = np.random.permutation(num_samples)\n",
    "    x = x[:, idx]\n",
    "    y = y[:, idx]\n",
    "\n",
    "    selected_samples = int(num_samples * 0.15)\n",
    "    x = x[:, :selected_samples]\n",
    "    y = y[:, :selected_samples]\n",
    "\n",
    "    for i in range(epochs + 1):\n",
    "        for j in range(num_batches):\n",
    "            start_idx = j * batch_size\n",
    "            end_idx = (j + 1) * batch_size\n",
    "\n",
    "            x_batch = x[:, start_idx:end_idx]\n",
    "            y_batch = y[:, start_idx:end_idx]\n",
    "\n",
    "            dw, db = 0, 0\n",
    "            if loss == 'mse':\n",
    "                dw += grad_w_mse(x_batch, y_batch, w, b) + l2_reg * w\n",
    "                db += grad_b_mse(x_batch, y_batch, w, b)\n",
    "            elif loss == 'cross_entropy':\n",
    "                dw += grad_w_cross(x_batch, y_batch, w, b) + l2_reg * w\n",
    "                db += grad_b_cross(x_batch, y_batch, w, b)\n",
    "\n",
    "            # Momentum\n",
    "            momentum_w = beta1 * momentum_w + (1 - beta1) * dw\n",
    "            momentum_b = beta1 * momentum_b + (1 - beta1) * db\n",
    "            # Update History\n",
    "            update_w = beta2 * update_w + (1 - beta2) * dw ** 2\n",
    "            update_b = beta2 * update_b + (1 - beta2) * db ** 2\n",
    "            # Bias Correction\n",
    "            momentum_w = momentum_w / (1 - beta1 ** (i + 1))\n",
    "            momentum_b = momentum_b / (1 - beta1 ** (i + 1))\n",
    "            update_w = update_w / (1 - beta2 ** (i + 1))\n",
    "            update_b = update_b / (1 - beta2 ** (i + 1))\n",
    "            # Update of Parameters\n",
    "            w = w - (lr / (np.sqrt(update_w) + epsilon)) * momentum_w\n",
    "            b = b - (lr / (np.sqrt(update_b) + epsilon)) * momentum_b\n",
    "\n",
    "        if loss == 'mse':\n",
    "            l = mse(x, y, w, b)\n",
    "            print(f'Loss after {i}th epoch = {l}')\n",
    "            l_list.append(l)\n",
    "        elif loss == 'cross_entropy':\n",
    "            l = cross_entropy(x, y, w, b)\n",
    "            print(f'Loss after {i}th epoch = {l}')\n",
    "            l_list.append(l)\n",
    "        w_list.append(w)\n",
    "        b_list.append(b)\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epoch Curve\\nAlgorithm: Mini-Batch Adam\\nBatch Size = {}\\nLearning Rate = {}\\nLoss Function = {}'.format(batch_size, lr, loss))\n",
    "    plt.plot(ep, l_list)\n",
    "    plt.show()\n",
    "\n",
    "    return w_list, b_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "275fe6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalize the input data\n",
    "mean = np.mean(x)\n",
    "std = np.std(x)\n",
    "x = (x - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e9fe51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Additional modifications:\n",
    "# 2. Apply L2 regularization (adjust the regularization strength as desired)\n",
    "l2_reg = 0.0001\n",
    "\n",
    "# 5. Reduce the batch size (adjust the batch size as desired)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c3955d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:60: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: overflow encountered in true_divide\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0th epoch = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\1155178784.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMiniBatchAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\2296613021.py\u001b[0m in \u001b[0;36mMiniBatchAdam\u001b[1;34m(x, y, epochs, batch_size, loss, lr, l2_reg)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[0mdw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_w_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml2_reg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0mdb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_b_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cross_entropy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[0mdw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_w_cross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ml2_reg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\3077819912.py\u001b[0m in \u001b[0;36mgrad_b_mse\u001b[1;34m(x, y, w, b)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrad_b_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\3077819912.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(x, w, b)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m'''Sigmoid Function'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "W, B = MiniBatchAdam(x, y, 500, batch_size, 'mse', 0.01, l2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "54bddea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:56: RuntimeWarning: overflow encountered in true_divide\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:57: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: overflow encountered in true_divide\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:55: RuntimeWarning: overflow encountered in double_scalars\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in multiply\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\Sony\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 0th epoch = nan\n",
      "Loss after 1th epoch = nan\n",
      "Loss after 2th epoch = nan\n",
      "Loss after 3th epoch = nan\n",
      "Loss after 4th epoch = nan\n",
      "Loss after 5th epoch = nan\n",
      "Loss after 6th epoch = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\1079934911.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMiniBatchAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\1806964585.py\u001b[0m in \u001b[0;36mMiniBatchAdam\u001b[1;34m(x, y, epochs, batch_size, loss, lr)\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mse'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0mdw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_w_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 \u001b[0mdb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_b_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cross_entropy'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8140\\3077819912.py\u001b[0m in \u001b[0;36mgrad_w_mse\u001b[1;34m(x, y, w, b)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrad_w_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W, B = MiniBatchAdam(x, y, 500, 10, 'mse', 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbdb7f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31511c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610f452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519a0ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8aca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787475a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad6b01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805111e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f59266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29d797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c974b3db",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'w1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6724\\2795393273.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m# Apply Adam optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6724\\2795393273.py\u001b[0m in \u001b[0;36madam_optimizer\u001b[1;34m(params, grads, learning_rate, beta1, beta2, epsilon)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;31m# Update parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparam_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Update biased first moment estimate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'w1'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def adam_optimizer(params, grads, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Adam optimization algorithm.\n",
    "\n",
    "    Args:\n",
    "    params: Dictionary containing the model's parameters.\n",
    "    grads: Dictionary containing the gradients of the parameters.\n",
    "    learning_rate: Learning rate for the optimization (default: 0.001).\n",
    "    beta1: Exponential decay rate for the first moment estimates (default: 0.9).\n",
    "    beta2: Exponential decay rate for the second moment estimates (default: 0.999).\n",
    "    epsilon: Small value to avoid division by zero (default: 1e-8).\n",
    "\n",
    "    Returns:\n",
    "    Updated parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize moments\n",
    "    v = {}\n",
    "    s = {}\n",
    "    for param_name, grad in grads.items():\n",
    "        v[param_name] = np.zeros_like(grad)\n",
    "        s[param_name] = np.zeros_like(grad)\n",
    "\n",
    "    # Update parameters\n",
    "    for param_name, param in params.items():\n",
    "        grad = grads[param_name]\n",
    "\n",
    "        # Update biased first moment estimate\n",
    "        v[param_name] = beta1 * v[param_name] + (1 - beta1) * grad\n",
    "\n",
    "        # Update biased second raw moment estimate\n",
    "        s[param_name] = beta2 * s[param_name] + (1 - beta2) * np.square(grad)\n",
    "\n",
    "        # Compute bias-corrected first and second moment estimates\n",
    "        v_corrected = v[param_name] / (1 - beta1)\n",
    "        s_corrected = s[param_name] / (1 - beta2)\n",
    "\n",
    "        # Update parameters\n",
    "        params[param_name] -= learning_rate * v_corrected / (np.sqrt(s_corrected) + epsilon)\n",
    "\n",
    "    return params\n",
    "\n",
    "# Example usage\n",
    "# Define your model parameters here\n",
    "params = {\n",
    "    'w1': np.random.randn(32, 32, 3, 64),\n",
    "    'b1': np.zeros((1, 64)),\n",
    "    'w2': np.random.randn(3, 3, 64, 128),\n",
    "    'b2': np.zeros((1, 128)),\n",
    "    # Add more parameters as needed\n",
    "}\n",
    "\n",
    "# Define your gradients here\n",
    "grads = {\n",
    "    'dw1': np.random.randn(32, 32, 3, 64),\n",
    "    'db1': np.zeros((1, 64)),\n",
    "    'dw2': np.random.randn(3, 3, 64, 128),\n",
    "    'db2': np.zeros((1, 128)),\n",
    "    # Add more gradients as needed\n",
    "}\n",
    "\n",
    "# Apply Adam optimizer\n",
    "params = adam_optimizer(params, grads, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17316c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,3072) (10,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6724\\1195874333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;31m#Error Surface MSE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6724\\1195874333.py\u001b[0m in \u001b[0;36mAdam\u001b[1;34m(x, y, epochs, batch_size, loss, lr)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[0mdw\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_w_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m                 \u001b[0mdb\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgrad_b_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"cross_entropy\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6724\\1195874333.py\u001b[0m in \u001b[0;36mgrad_w_mse\u001b[1;34m(x, y, w, b)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrad_w_mse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mfx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,3072) (10,10) "
     ]
    }
   ],
   "source": [
    "x = x_train\n",
    "y = y_train\n",
    "\n",
    "\n",
    "#Helper Functions\n",
    "def f(x,w,b):\n",
    "    '''Sigmoid Function'''\n",
    "    f = 1/(1+np.exp(-(w*x+b)))\n",
    "    return f\n",
    "def mse(x,y,w,b):\n",
    "    '''Mean Squared Loss Function'''\n",
    "    L = 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        L += 0.5*(y[i]-f(x[i],w,b))**2\n",
    "    return L\n",
    "def cross_entropy(x,y,w,b):\n",
    "    '''Cross Entropy Loss Function'''\n",
    "    L = 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        L += -(y[i]*np.log(f(x[i],w,b)))\n",
    "    return L\n",
    "def grad_w_mse(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    dw = np.dot((fx - y) * fx * (1 - fx), x.T)\n",
    "    return dw\n",
    "\n",
    "def grad_b_mse(x, y, w, b):\n",
    "    fx = f(x, w, b)\n",
    "    db = np.sum((fx - y) * fx * (1 - fx), axis=1, keepdims=True)\n",
    "    return db\n",
    "\n",
    "def grad_w_cross(x,y,w,b):\n",
    "    fx = f(x,w,b) \t\n",
    "    dw = (- y)*(1-fx)*x\n",
    "    return dw\n",
    "def grad_b_cross(x,y,w,b):\n",
    "    fx = f(x,w,b) \n",
    "    db = (- y)*(1-fx)\n",
    "    return db\n",
    "\n",
    "#Gradient Discent\n",
    "def Adam(x,y,epochs,batch_size,loss,lr):\n",
    "    w = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    epsilon = 1e-8\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    momentum_w,momentum_b = 0,0\n",
    "    update_w, update_b = 0,0\n",
    "    l_list = []\n",
    "    w_list = []\n",
    "    b_list = []\n",
    "    points = 0\n",
    "    ep = [i for i in range(epochs+1)]\n",
    "    dw,db = np.zeros_like(w), np.zeros_like(b)\n",
    "\n",
    "    for i in range(epochs + 1):\n",
    "        dw, db = np.zeros_like(w), np.zeros_like(b)\n",
    "        for j in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[j : j + batch_size]\n",
    "            y_batch = y[j : j + batch_size]\n",
    "\n",
    "            if loss == \"mse\":\n",
    "                dw += grad_w_mse(x_batch, y_batch, w, b)\n",
    "                db += grad_b_mse(x_batch, y_batch, w, b)\n",
    "            elif loss == \"cross_entropy\":\n",
    "                # ...\n",
    "                pass\n",
    "\n",
    "            points += 1\n",
    "            if(points % batch_size == 0):\n",
    "                #Momentum\n",
    "                momentum_w = beta1 * momentum_w + (1 - beta1) * dw\n",
    "                momentum_b = beta1 * momentum_b + (1 - beta1) * db\n",
    "                #Update History\n",
    "                update_w = beta2 * update_w + (1 - beta2) * dw**2\n",
    "                update_b = beta2 * update_b + (1 - beta2) * db**2 \n",
    "                #Bias Correction\n",
    "                momentum_w = momentum_w /(1 - math.pow(beta1,i+1))  \n",
    "                momentum_b = momentum_b /(1 - math.pow(beta1,i+1))\n",
    "                update_w = update_w /(1 - math.pow(beta2,i+1))  \n",
    "                update_b = update_b /(1 - math.pow(beta2,i+1))\n",
    "                #Update of Parameters\n",
    "                w = w - (lr/np.sqrt(update_w + epsilon))*momentum_w\n",
    "                b = b - (lr/np.sqrt(update_b + epsilon))*momentum_b\n",
    "                dw,db = 0,0\n",
    "        if (loss == 'mse'):\n",
    "            print('Loss after {}th epoch = {}\\n'.format(i,mse(x,y,w,b)[0]))\n",
    "            l_list.append(mse(x,y,w,b)[0])\n",
    "        elif (loss == 'cross_entropy'):\n",
    "            print('Loss after {}th epoch = {}\\n'.format(i,cross_entropy(x,y,w,b)[0]))\n",
    "            l_list.append(cross_entropy(x,y,w,b)[0])\n",
    "        w_list.append(w[0])\n",
    "        b_list.append(b[0])\n",
    "\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss vs Epoch Curve\\nAlgotithm :Mini Batch Adam\\nBatch Size = {}\\nInitial Learning Rate = {}\\nLoss Function = {}'.format(batch_size,lr,loss))\n",
    "    plt.plot(ep,l_list)\n",
    "    plt.show()\n",
    "\n",
    "    return w_list,b_list\n",
    "\n",
    "\n",
    "x = x_train.reshape(x_train.shape[0], -1)\n",
    "y = y_train\n",
    "\n",
    "W,B = Adam(x,y,500,10,'mse',0.01)\n",
    "\n",
    "#Error Surface MSE\n",
    "w = np.linspace(-10,10,num = 1000,dtype = np.float)\n",
    "b = np.linspace(-10,10,num = 1000,dtype = np.float)\n",
    "w,b = np.meshgrid(w,b)\n",
    "\n",
    "mse_list = []\n",
    "for i in range(w.shape[0]):\n",
    "    Loss = mse(x,y,w[i],b[i])\n",
    "    mse_list.append(Loss)\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(w, b, mse_list, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "plt.title('MSE Error Suface')\n",
    "plt.show()\n",
    "\n",
    "#Error Surface Cross Entropy\n",
    "cross_list = []\n",
    "for i in range(w.shape[0]):\n",
    "    Loss = cross_entropy(x,y,w[i],b[i])\n",
    "    cross_list.append(Loss)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(w, b, cross_list, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "plt.title('Cross Entropy Error Suface')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049f1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
